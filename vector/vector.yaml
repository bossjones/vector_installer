# Set global options
data_dir: "/var/lib/vector"

# Vector's API (disabled by default)
# Enable and try it out with the `vector top` command
api:
  enabled: true
  address: "0.0.0.0:8686"

log_schema:
  timestamp_key: vector_timestamp
  # host_key: log_host


# Ingest data by tailing one or more files
sources:
  vector_metrics: # internal metrics
    type: "internal_metrics"

  vector_logs:
    type: "internal_logs"

  syslog_logs:
    type: "file"
    include:
      - "/var/log/syslog"
      - "/var/log/mail.info"
      - "/var/log/mail.warn"
      - "/var/log/mail.err"
      - "/var/log/mail.log"
      - "/var/log/daemon.log"
      - "/var/log/kern.log"
      - "/var/log/auth.log"
      - "/var/log/user.log"
      - "/var/log/lpr.log"
      - "/var/log/cron.log"
      - "/var/log/debug"
      - "/var/log/messages"
    ignore_older: 86400          # 1 day
    # The delay between file discovery calls.
    # This controls the interval at which files are searched. A higher value results in greater chances of some short-lived files being missed between searches, but a lower value increases the performance impact of file discovery.
    glob_minimum_cooldown_ms: 1000
    # host_key: host
    # Ignore files with a data modification date older than the specified number of seconds.
    # ignore_older_secs: 600
    # line_delimiter: "\n"
    max_line_bytes: 102400
    max_read_bytes: 2048
    # offset_key: offset
    # read_from: beginning

  auth_logs:
    type: "file"
    include:
      - "/var/log/auth.log" # supports globbing
    ignore_older: 86400          # 1 day
    # The delay between file discovery calls.
    # This controls the interval at which files are searched. A higher value results in greater chances of some short-lived files being missed between searches, but a lower value increases the performance impact of file discovery.
    glob_minimum_cooldown_ms: 1000
    # host_key: host
    # Ignore files with a data modification date older than the specified number of seconds.
    # ignore_older_secs: 600
    # line_delimiter: "\n"
    max_line_bytes: 102400
    max_read_bytes: 2048
    # offset_key: offset
    # read_from: beginning

  # var_log_wildcard_logs:
  #   type: "file"
  #   include:
  #     - "/var/log/**/*log" # supports globbing
  #   ignore_older: 86400          # 1 day
  #   # The delay between file discovery calls.
  #   # This controls the interval at which files are searched. A higher value results in greater chances of some short-lived files being missed between searches, but a lower value increases the performance impact of file discovery.
  #   glob_minimum_cooldown_ms: 1000
  #   # host_key: host
  #   # Ignore files with a data modification date older than the specified number of seconds.
  #   # ignore_older_secs: 600
  #   # line_delimiter: "\n"
  #   max_line_bytes: 102400
  #   max_read_bytes: 2048
  #   # offset_key: offset
  #   # read_from: beginning

  # var_log_logs:
  #   type: "file"
  #   include:
  #     - "/var/log/*log" # supports globbing
  #   ignore_older: 86400          # 1 day
  #   # The delay between file discovery calls.
  #   # This controls the interval at which files are searched. A higher value results in greater chances of some short-lived files being missed between searches, but a lower value increases the performance impact of file discovery.
  #   glob_minimum_cooldown_ms: 1000
  #   # host_key: host
  #   # Ignore files with a data modification date older than the specified number of seconds.
  #   # ignore_older_secs: 600
  #   # line_delimiter: "\n"
  #   max_line_bytes: 102400
  #   max_read_bytes: 2048
  #   # offset_key: offset
  #   # read_from: beginning

  # proxmox_task_logs:
  #   type: "file"
  #   include:
  #     - "/var/log/pve/tasks/**/*" # supports globbing
  #   exclude:
  #     - "/var/log/pve/tasks/5"
  #     - "/var/log/pve/tasks/4"
  #     - "/var/log/pve/tasks/A"
  #     - "/var/log/pve/tasks/B"
  #     - "/var/log/pve/tasks/6"
  #     - "/var/log/pve/tasks/3"
  #     - "/var/log/pve/tasks/D"
  #     - "/var/log/pve/tasks/7"
  #     - "/var/log/pve/tasks/F"
  #     - "/var/log/pve/tasks/E"
  #     - "/var/log/pve/tasks/2"
  #     - "/var/log/pve/tasks/0"
  #     - "/var/log/pve/tasks/C"
  #     - "/var/log/pve/tasks/1"
  #     - "/var/log/pve/tasks/9"
  #     - "/var/log/pve/tasks/8"
  #   ignore_older: 86400          # 1 day
  #   # The delay between file discovery calls.
  #   # This controls the interval at which files are searched. A higher value results in greater chances of some short-lived files being missed between searches, but a lower value increases the performance impact of file discovery.
  #   glob_minimum_cooldown_ms: 1000
  #   # host_key: host
  #   # Ignore files with a data modification date older than the specified number of seconds.
  #   # ignore_older_secs: 600
  #   # line_delimiter: "\n"
  #   max_line_bytes: 102400
  #   max_read_bytes: 2048
  #   # offset_key: offset
  #   # read_from: beginning

  journald:
    type: journald
    # batch_size: 16
    # current_boot_only: true
    # data_dir: /var/lib/vector
    # exclude_matches:
    #   _SYSTEMD_UNIT:
    #     - sshd.service
    #     - ntpd.service
    #   _TRANSPORT:
    #     - kernel
    # exclude_units:
    #   - badservice
    # extra_args:
    #   - --merge
    # include_matches:
    #   _SYSTEMD_UNIT:
    #     - sshd.service
    #     - ntpd.service
    #   _TRANSPORT:
    #     - kernel
    # include_units:
    #   - ntpd

# Structure and parse via Vector's Remap Language
transforms:
  syslog_parser:
    inputs:
      - "syslog_logs"
    type: "remap"
    source: ". |= parse_syslog!(.message)"

  auth_parser:
    inputs:
      - "auth_logs"
    type: "remap"
    source: |
      . |= parse_linux_authorization!(.message)
    # if err != null {
    #   log("Unable to parse linux authorization: " + err, level: "error")
    # } else {
    #   . = merge(., structured)
    # }

  # # Sample the data to save on cost
  # apache_sampler:
  #   inputs:
  #     - "apache_parser"
  #   type: "sample"
  #   rate: 2 # only keep 50% (1/`rate`)

  # vector_parser_modify:
  #   inputs:
  #     - "vector_logs"
  #   type: remap
  #   # Reformat the timestamp to Unix time
  #   # ".timestamp = to_unix_timestamp!(to_timestamp!(.timestamp))"
  #   source: |
  #     matches, err = parse_regex(
  #       .message,
  #       r'^(?P<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{9}) (?P<message>.*)'
  #     )
  #     if matches != null {
  #       .message = matches.message
  #       .@timestamp = parse_timestamp!(matches.time, "%F %T%.9f")
  #     } else {
  #       log(err, level: "error")
  #       .malformed = true
  #     }

  vector_parser_modify:
    inputs:
      - "vector_logs"
    type: remap
    # Reformat the timestamp to Unix time
    # ".timestamp = to_unix_timestamp!(to_timestamp!(.timestamp))"
    # .@timestamp = parse_timestamp!(.timestamp, "%F %T%.9f")
    source: |
      .malformed = false
      matches, err = parse_regex(
        .message,
        r'^(?P<message>.*)'
      )
      if matches != null {
        .message = matches.message
        .@timestamp = parse_timestamp!(.timestamp, "%F %T%.9f")
      } else {
        log(err, level: "error")
        .malformed = true
      }

  vector_parser_add_timestamp:
    inputs:
      - "vector_logs"
    type: remap
    # Reformat the timestamp to Unix time
    # ".timestamp = to_unix_timestamp!(to_timestamp!(.timestamp))"
    source: |
      .malformed = false
      matches, err = parse_regex(
        .message,
        r'^(?P<message>.*)'
      )
      if matches != null {
        .message = matches.message
        .@timestamp = now()
      } else {
        log(err, level: "error")
        .malformed = true
      }

  vector_parser_modify_malformed_message_filter:
    inputs:
      - vector_parser_modify
    type: filter
    condition: .malformed != true


  # catchall_parser:
  #   type: remap
  #   inputs:
  #     - vector_logs
  #   source: |
  #     .agent_name = "vector"
  #     parsed, err = parse_json(.message)
  #     if err == null {
  #         .message = parsed
  #         .format = "json"
  #     } else {
  #         .format = "ascii"
  #     }
  #     matches, err = parse_regex!(.file, r'.*/(?P<num>\d+)-(?P<name>\w+).log')
  #     .origin, err = .host + "/" + matches.name + "/" + matches.num
  #     if err == null {
  #         log("Failed to parse origin from file name", level: "error")
  #     }

  # proxmox_task_logs_reduce:
  #   type: reduce
  #   inputs:
  #     - "proxmox_task_logs"
  #     - "var_log_wildcard_logs"

  # # SOURCE: https://github.com/mitodl/salt-ops/blob/f9b4bd696151587704004c93aa01076327ade91d/salt/vector/templates/ocw_build_logs.yaml#L6
  # # webhook_publish_log_parser:
  # #   inputs:
  # #     - webhook_publish_log
  # #   type: remap
  # #   source: |
  # #     matches, err = parse_regex(
  # #       .message,
  # #       r'^(?P<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{9}) (?P<message>.*)'
  # #     )
  # #     if matches != null {
  # #       .message = matches.message
  # #       .@timestamp = parse_timestamp!(matches.time, "%F %T%.9f")
  # #       .labels = ["ocw_build"]
  # #       .environment = "{{ config_elements.environment }}"
  # #     } else {
  # #       log(err, level: "error")
  # #       .malformed = true
  # #     }

  # # SOURCE: https://vector.dev/docs/reference/configuration/
  # add_host_parser:
  #   type: "remap"
  #   source: |
  #     # Basic usage. "$HOSTNAME" also works.
  #     .host = "${HOSTNAME}" # or "$HOSTNAME"
  #     .nodename = "${HOSTNAME}" # or "$HOSTNAME"
  #     .instance = "${HOSTNAME}" # or "$HOSTNAME"

  #     .cluster = "k3d"
  #   inputs:
  #     - "proxmox_task_logs"
  #     - "var_log_logs"
  #     - "var_log_wildcard_logs"


# Send structured data to a short-term storage
sinks:
  # ----------------------------------------------------------------------
  # debugging
  # ----------------------------------------------------------------------
  # # console:
  # #   type: "console"
  # #   inputs:
  # #     # - "vector_logs"
  # #     # - "add_host_parser"
  # #     # - "proxmox_task_logs_reduce"
  # #     - "vector_parser_modify_malformed_message_filter"
  # #   encoding:
  # #     codec: "text"

  # vector_logs_test_file_sink:
  #   type: file
  #   inputs:
  #     - "vector_parser_modify_malformed_message_filter"
  #   compression: none
  #   idle_timeout_secs: 30
  #   path: /tmp/vector-%Y-%m-%d.log
  #   encoding:
  #     codec: "text"

  # journald_test_file_sink:
  #   type: file
  #   inputs:
  #     - "journald"
  #   compression: none
  #   idle_timeout_secs: 30
  #   path: /tmp/journald-%Y-%m-%d.log
  #   encoding:
  #     codec: "text"

  # vector_parser_add_timestamp_test_file_sink:
  #   type: file
  #   inputs:
  #     - "vector_parser_add_timestamp"
  #   compression: none
  #   idle_timeout_secs: 30
  #   path: /tmp/vector-timestamp-%Y-%m-%d.log
  #   encoding:
  #     codec: "text"

  # vector_parser_add_timestamp_test_file_sink_json:
  #   type: file
  #   inputs:
  #     - "vector_parser_add_timestamp"
  #   compression: none
  #   idle_timeout_secs: 30
  #   path: /tmp/vector-timestamp-json-%Y-%m-%d.log
  #   encoding:
  #     codec: "json"

  # catchall_parser_add_timestamp_test_file_sink_json:
  #   type: file
  #   inputs:
  #     - "catchall_parser"
  #   compression: none
  #   idle_timeout_secs: 30
  #   path: /tmp/catchall-timestamp-json-%Y-%m-%d.log
  #   encoding:
  #     codec: "json"

  # syslog_test_file_sink_json:
  #   type: file
  #   inputs:
  #     - "syslog_parser"
  #   compression: none
  #   idle_timeout_secs: 30
  #   path: /tmp/syslog-timestamp-json-%Y-%m-%d.log
  #   encoding:
  #     codec: "json"

  # daemon_test_file_sink_json:
  #   type: file
  #   inputs:
  #     - "daemon_parser"
  #   compression: none
  #   idle_timeout_secs: 30
  #   path: /tmp/daemon-timestamp-json-%Y-%m-%d.log
  #   encoding:
  #     codec: "json"

  # https://vector.dev/docs/reference/configuration/sinks/loki/#label-expansion
  loki:
    type: loki
    inputs:
      # - "proxmox_task_logs_reduce"
      # - "add_host_parser"
      # - "vector_parser_modify_malformed_message_filter"
      - "vector_parser_add_timestamp"
      - "vector_parser_modify_malformed_message_filter"
      - "journald"
      - "syslog_parser"
      - "auth_parser"
    # endpoint: http://localhost:3100
    endpoint: http://loki-gateway.scarlettlab.home
    # compression: snappy
    labels:
      # '"*"': "{{ metadata }}"
    #   '"pod_labels_*"': "{{ kubernetes.pod_labels }}"
      forwarder: vector
    #   "{{ event_field }}": "{{ some_other_event_field }}"
    out_of_order_action: accept
    # path: /loki/api/v1/push
    remove_timestamp: true
    remove_label_fields: false
    tenant_id: docker
    healthcheck:
      enabled: true
    request:
      concurrency: "adaptive"
      rate_limit_duration_secs: 1
      rate_limit_num: 10
    encoding:
      codec: "json"
      timestamp_format: "rfc3339"
    auth:
      strategy: "basic"
      user: ""
      password: ""
    # timestamp_format: "rfc3339"

  # prometheus:
  #   type: "prometheus_remote_write"
  #   inputs:
  #     - "vector_metrics"
  #   endpoint: "http://pushgateway.scarlettlab.home/api/prom/push"
  #   healthcheck: false

  prom_exporter:
    type: prometheus_exporter
    inputs:
      - "vector_metrics"
    address: "0.0.0.0:9598"
